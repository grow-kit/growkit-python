# domains/evaluation/service.py
#whisper ëª¨ë¸ì„ ë¶ˆëŸ¬ì™€ì„œ binary ì˜¤ë””ì˜¤ ë°ì´í„°ë¥¼ ë°›ì•„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜í•˜ëŠ” í•µì‹¬ ë¡œì§

import os
import cv2
from faster_whisper import WhisperModel
from moviepy.editor import VideoFileClip
import tempfile

# Whisper ëª¨ë¸ (ë¹ ë¥¸ ë²„ì „)
stt_model = WhisperModel("base", device="cpu", compute_type="int8")

# Haar Cascade ë¡œë“œ
face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + "haarcascade_frontalface_default.xml")
eye_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + "haarcascade_eye.xml")


# ðŸŽ™ï¸ ì˜ìƒì—ì„œ ìŒì„± ì¶”ì¶œ â†’ STT í…ìŠ¤íŠ¸ ë³€í™˜
def transcribe_audio_from_video(video_path: str) -> str:
    audio_path = video_path.replace(".mp4", ".wav")
    clip = VideoFileClip(video_path)
    clip.audio.write_audiofile(audio_path, verbose=False, logger=None)
    clip.close()  # âœ… íŒŒì¼ ì ìœ  í•´ì œ

    segments, _ = stt_model.transcribe(audio_path, language="ko")
    text = " ".join([segment.text for segment in segments])

    os.remove(audio_path)
    return text


# ðŸ‘ï¸ ì‹œì„  + ê³ ê°œ ì›€ì§ìž„ ë¶„ì„
def analyze_pose_only(video_path: str) -> dict:
    cap = cv2.VideoCapture(video_path)
    gaze_directions = []
    yaw_distances = []

    while True:
        ret, frame = cap.read()
        if not ret:
            break

        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
        faces = face_cascade.detectMultiScale(gray, 1.1, 4)

        for (x, y, w, h) in faces:
            roi = gray[y:y + h, x:x + w]
            eyes = eye_cascade.detectMultiScale(roi)

            if len(eyes) >= 2:
                centers = [(ex + ew // 2) for (ex, ey, ew, eh) in eyes]
                avg_x = sum(centers) / len(centers)

                if avg_x < w // 2 - 10:
                    gaze_directions.append("ì™¼ìª½")
                elif avg_x > w // 2 + 10:
                    gaze_directions.append("ì˜¤ë¥¸ìª½")
                else:
                    gaze_directions.append("ì •ë©´")

                yaw = abs(centers[0] - centers[1])
                yaw_distances.append(yaw)

    cap.release()

    gaze_result = max(set(gaze_directions), key=gaze_directions.count) if gaze_directions else "ì•Œ ìˆ˜ ì—†ìŒ"
    head_motion = (
        "ì›€ì§ìž„ ìžˆìŒ" if max(yaw_distances, default=0) - min(yaw_distances, default=0) > 10 else "ì•ˆì •ì "
    )

    return {
        "gaze_direction": gaze_result,
        "head_stability": head_motion
    }


# ðŸ”„ ì „ì²´ ë¶„ì„ í†µí•©
def analyze_video_all(binary_video: bytes) -> dict:
    with tempfile.NamedTemporaryFile(delete=False, suffix=".mp4") as temp_file:
        temp_file.write(binary_video)
        temp_file.flush()
        video_path = temp_file.name

    try:
        stt_text = transcribe_audio_from_video(video_path)
        pose_result = analyze_pose_only(video_path)
    finally:
        os.remove(video_path)

    return {
        "text": stt_text,
        "gaze_direction": pose_result["gaze_direction"],
        "head_motion": pose_result["head_stability"]
    }


