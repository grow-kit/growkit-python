# domains/evaluation/service.py
#whisper ëª¨ë¸ì„ ë¶ˆëŸ¬ì™€ì„œ binary ì˜¤ë””ì˜¤ ë°ì´í„°ë¥¼ ë°›ì•„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜í•˜ëŠ” í•µì‹¬ ë¡œì§

import os
import cv2
from faster_whisper import WhisperModel
from moviepy.editor import VideoFileClip
import tempfile
import subprocess

# ì˜ìƒ ì²˜ë¦¬ ë¡œì§
def fix_video_metadata(input_path: str) -> str:
    output_path = input_path.replace(".mp4", "_fixed.mp4")
    # ğŸ” ì‹¤ì œ ì»¨í…Œì´ë„ˆ ê²€ì‚¬: ì›¹Mì´ë©´ mp4ë¡œ ë³€í™˜
    if input_path.endswith(".webm"):
        output_path = input_path.replace(".webm", "_converted.mp4")
        cmd = [
            "ffmpeg", "-i", input_path,
            "-c:v", "libx264", "-c:a", "aac",
            "-movflags", "faststart",
            output_path
        ]
    else:
        cmd = [
            "ffmpeg", "-i", input_path,
            "-c:v", "copy", "-c:a", "copy",
            "-movflags", "faststart",
            output_path
        ]

    subprocess.run(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
    return output_path
#end def

# Whisper ëª¨ë¸ (ë¹ ë¥¸ ë²„ì „)
stt_model = WhisperModel("base", device="cpu", compute_type="int8")

# Haar Cascade ë¡œë“œ
face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + "haarcascade_frontalface_default.xml")
eye_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + "haarcascade_eye.xml")


# ğŸ™ï¸ ì˜ìƒì—ì„œ ìŒì„± ì¶”ì¶œ â†’ STT í…ìŠ¤íŠ¸ ë³€í™˜
def transcribe_audio_from_video(video_path: str) -> str:
    fixed_path = fix_video_metadata(video_path)
    audio_path = video_path.replace(".mp4", ".wav")

    clip = VideoFileClip(fixed_path)
    clip.audio.write_audiofile(audio_path, verbose=False, logger=None)
    clip.close()  # âœ… íŒŒì¼ ì ìœ  í•´ì œ

    segments, _ = stt_model.transcribe(audio_path, language="ko")
    text = " ".join([segment.text for segment in segments])

    os.remove(audio_path)
    os.remove(fixed_path)  # âœ… ì¬ì¸ì½”ë”©ëœ ì˜ìƒë„ ì •ë¦¬
    return text


# ğŸ‘ï¸ ì‹œì„  + ê³ ê°œ ì›€ì§ì„ ë¶„ì„
def analyze_pose_only(video_path: str) -> dict:
    cap = cv2.VideoCapture(video_path)
    gaze_directions = []
    yaw_distances = []

    while True:
        ret, frame = cap.read()
        if not ret:
            break

        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
        faces = face_cascade.detectMultiScale(gray, 1.1, 4)

        for (x, y, w, h) in faces:
            roi = gray[y:y + h, x:x + w]
            eyes = eye_cascade.detectMultiScale(roi)

            if len(eyes) >= 2:
                centers = [(ex + ew // 2) for (ex, ey, ew, eh) in eyes]
                avg_x = sum(centers) / len(centers)

                if avg_x < w // 2 - 10:
                    gaze_directions.append("ì™¼ìª½")
                elif avg_x > w // 2 + 10:
                    gaze_directions.append("ì˜¤ë¥¸ìª½")
                else:
                    gaze_directions.append("ì •ë©´")

                yaw = abs(centers[0] - centers[1])
                yaw_distances.append(yaw)

    cap.release()

    gaze_result = max(set(gaze_directions), key=gaze_directions.count) if gaze_directions else "ì•Œ ìˆ˜ ì—†ìŒ"
    head_motion = (
        "ì›€ì§ì„ ìˆìŒ" if max(yaw_distances, default=0) - min(yaw_distances, default=0) > 10 else "ì•ˆì •ì "
    )

    return {
        "gaze_direction": gaze_result,
        "head_stability": head_motion
    }


# ğŸ”„ ì „ì²´ ë¶„ì„ í†µí•©
def analyze_video_all(binary_video: bytes) -> dict:
    with tempfile.NamedTemporaryFile(delete=False, suffix=".mp4") as temp_file:
        temp_file.write(binary_video)
        temp_file.flush()
        video_path = temp_file.name

    stt_text = "ìŒì„± ì¸ì‹ ì‹¤íŒ¨"
    pose_result = {
        "gaze_direction": "ì•Œ ìˆ˜ ì—†ìŒ",
        "head_stability": "ì•Œ ìˆ˜ ì—†ìŒ"
    }

    try:
        try:
            stt_text = transcribe_audio_from_video(video_path)
        except Exception as e:
            print(f"ğŸ™ï¸ ìŒì„± ë¶„ì„ ì‹¤íŒ¨: {e}")

        try:
            pose_result = analyze_pose_only(video_path)
        except Exception as e:
            print(f"ğŸ‘ï¸ ì‹œì„  ë¶„ì„ ì‹¤íŒ¨: {e}")

    finally:
        os.remove(video_path)

    return {
        "text": stt_text,
        "gaze_direction": pose_result.get("gaze_direction", "ì•Œ ìˆ˜ ì—†ìŒ"),
        "head_motion": pose_result.get("head_stability", "ì•Œ ìˆ˜ ì—†ìŒ")
    }




