# domains/evaluation/router.py
# ì‹¤ì œ HTTP ìš”ì²­ì„ ì²˜ë¦¬í•˜ëŠ” ì—”ë“œí¬ì¸íŠ¸ ì •ì˜
# Controller ì—­í• 

import os
from fastapi import APIRouter, File, UploadFile, HTTPException, Form
from domains.evaluation.schemas import EvaluationRequest, AnalysisResult
from core.gpt_engine import generate_question_with_manual, fetch_manual, fetch_criteria,generate_feedback_with_criteria
from domains.evaluation.service import analyze_video_all

router = APIRouter()


# @router.post("/test", response_model=TranscriptionResult)
# async def test(file: UploadFile = File(...)):
#     binary = await file.read()
#     ext = os.path.splitext(file.filename)[-1] or ".mp3"
#     text = transcribe_audio(binary, suffix=ext)
#     return TranscriptionResult(text=text)
# end def


# ë¬¸í•­ ìƒì„± ìš”ì²­
@router.get("/generate-question/{manual_id}")
async def get_question(manual_id: int):
    try:
        question = await generate_question_with_manual(manual_id)
        return {"question": question}
    except HTTPException as e:
        # FastAPIì— ë‹¤ì‹œ ì˜ˆì™¸ ì „ë‹¬
        raise e
# end def


# # ë¶„ì„ ë° í”¼ë“œë°± ìš”ì²­
# @router.post("/analyze-response")
# async def analyze_response(request: EvaluationRequest):
#     result = generate_feedback(request.question, request.answer, request.emotion)
#     return result
# # end def


@router.post("/audio-video", response_model=AnalysisResult)
async def analyze_from_single_video(video: UploadFile = File(...)):
    binary = await video.read()
    result = analyze_video_all(binary)
    return AnalysisResult(
        text=result["text"],
        head_pose={
            "head_yaw": result["gaze_direction"],
            "head_pitch": result["head_motion"]
        }
    )


@router.post("/submit-answer")
async def submit_answer(
    video: UploadFile,
    question: str = Form(...),
    manual_id: int = Form(...),
    criteria_id: int = Form(...)
):
    binary = await video.read()

    analysis = analyze_video_all(binary)
    answer = analysis.get("text", "").strip()
    emotion_data = {
        "gaze": analysis.get("gaze_direction", "ì•Œ ìˆ˜ ì—†ìŒ"),
        "head": analysis.get("head_motion", "ì•Œ ìˆ˜ ì—†ìŒ")
    }

    # ğŸ¯ answerê°€ ë¹„ì •ìƒì¼ ê²½ìš° ê³ ì • ì‘ë‹µ
    if not answer or answer == "ìŒì„± ì¸ì‹ ì‹¤íŒ¨" or len(answer) < 5:
        return {
            "question": question,
            "answer": answer,
            "gaze": emotion_data["gaze"],
            "head": emotion_data["head"],
            "score": {},
            "feedback": "âš ï¸ ë‹µë³€ì´ ì •ìƒì ìœ¼ë¡œ ì¸ì‹ë˜ì§€ ì•Šì•„ í‰ê°€ê°€ ë¶ˆê°€ëŠ¥í•©ë‹ˆë‹¤. ë¬¸ì˜ í›„ ì¬í‰ê°€ ìš”ì²­ì„ ì§„í–‰í•´ ì£¼ì„¸ìš”."
        }

    manual = await fetch_manual(manual_id)
    criteria = await fetch_criteria(criteria_id)

    gpt_result = generate_feedback_with_criteria(
        question, answer, emotion_data, manual, criteria
    )

    return {
        "question": question,
        "answer": answer,
        "gaze": emotion_data["gaze"],
        "head": emotion_data["head"],
        "score": gpt_result["score"],
        "feedback": gpt_result["feedback"]
    }