# domains/simulation/router.py
from fastapi import APIRouter, HTTPException
from pydantic import BaseModel
from typing import List, Dict, Any
from openai import OpenAI, OpenAIError
import json
import os
import re

router = APIRouter()

# OpenAI API í‚¤ ì„¤ì • ë° ê²€ì¦
api_key = os.getenv("OPENAI_API_KEY")
if not api_key:
    print("ê²½ê³ : OPENAI_API_KEY í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    client = None
else:
    try:
        client = OpenAI(api_key=api_key)
    except Exception as e:
        print(f"OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        client = None

# Pydantic ëª¨ë¸ ì •ì˜
class HintsRequest(BaseModel):
    scenarios: List[Dict[str, Any]]
    task: str

class EducationalAnalysisRequest(BaseModel):
    userid: str
    companyid: int
    userorder: List[int]  
    reason: str
    responseTexts: Dict[str, str] 
    hints: Dict[str, str] 
    orderSelectionTime: int
    reasonWritingTime: int
    totalTimeSpent: int
    
    # âœ… ì¶”ê°€: ì‹œë‚˜ë¦¬ì˜¤ ì •ë³´
    scenarioContents: Dict[str, str] = {}  
    scenarioTags: Dict[str, str] = {}      

# ë¬´ì˜ë¯¸í•œ ì…ë ¥ ê²€ì¦ í•¨ìˆ˜
def validate_response_text(text: str) -> bool:
    """ì‘ë‹µ í…ìŠ¤íŠ¸ ìœ íš¨ì„± ê²€ì¦"""
    if not text or len(text.strip()) < 5:
        return False
    
    text_stripped = text.strip()
    
    # ììŒ/ëª¨ìŒë§Œ ìˆëŠ” ê²½ìš° (ã„±, ã…, ã…€ ë“±)
    korean_consonants = re.compile(r'^[ã„±-ã…ã…-ã…£]+$')
    if korean_consonants.match(text_stripped):
        return False
    
    # ë°˜ë³µ ë¬¸ì (ã…‹ã…‹ã…‹, ã…ã…ã…, 111, aaa ë“±)
    repeated_char = re.compile(r'^(.)\1{2,}$')
    if repeated_char.match(text_stripped):
        return False
    
    # ìˆ«ìë§Œ ìˆëŠ” ê²½ìš°
    if text_stripped.isdigit():
        return False
    
    # íŠ¹ìˆ˜ë¬¸ìë§Œ ìˆëŠ” ê²½ìš°
    if re.match(r'^[^a-zA-Z0-9ê°€-í£]+$', text_stripped):
        return False
    
    # ì˜ë¯¸ì—†ëŠ” ì§§ì€ ë°˜ë³µ (ã„±ã„±ã„±, ì•ˆì•ˆì•ˆ, ë„¤ë„¤ë„¤ ë“±)
    if len(text_stripped) <= 10:
        # ê°™ì€ ë¬¸ìê°€ 50% ì´ìƒì¸ ê²½ìš°
        char_count = {}
        for char in text_stripped:
            char_count[char] = char_count.get(char, 0) + 1
        
        max_char_count = max(char_count.values())
        if max_char_count / len(text_stripped) > 0.5:
            return False
    
    return True

def get_invalid_responses(request: EducationalAnalysisRequest) -> List[str]:
    """ë¬´ì˜ë¯¸í•œ ì‘ë‹µì´ ìˆëŠ” ì‹œë‚˜ë¦¬ì˜¤ ID ëª©ë¡ ë°˜í™˜"""
    invalid_responses = []
    for scenario_id, text in request.responseTexts.items():
        if not validate_response_text(text):
            invalid_responses.append(scenario_id)
    return invalid_responses

# ì‹œë‚˜ë¦¬ì˜¤ ë‚´ìš© ë§¤í•‘ í•¨ìˆ˜ - âœ… ìˆ˜ì •ëœ ë²„ì „
def get_scenario_info_by_id(scenario_id: int, request: EducationalAnalysisRequest = None) -> Dict[str, str]:
    """ì‹¤ì œ ì‹œë‚˜ë¦¬ì˜¤ IDë¥¼ ë°›ì•„ì„œ ë‚´ìš©ê³¼ íƒœê·¸ ë°˜í™˜"""
    
    # âœ… ìš”ì²­ì—ì„œ ì‹¤ì œ ì‹œë‚˜ë¦¬ì˜¤ ì •ë³´ê°€ ìˆìœ¼ë©´ ì‚¬ìš©
    if request and request.scenarioContents:
        scenario_id_str = str(scenario_id)
        if scenario_id_str in request.scenarioContents:
            return {
                "content": request.scenarioContents[scenario_id_str],
                "tags": request.scenarioTags.get(scenario_id_str, "")
            }
    
    # âŒ ê¸°ì¡´ ì¶”ì¸¡ ë¡œì§ (ë°±ì—…ìš©ìœ¼ë¡œë§Œ ì‚¬ìš©)
    scenario_patterns = {
        "coffee": {"content": "ì»¤í”¼ë¨¸ì‹ ì´ ì‘ë™í•˜ì§€ ì•ŠìŒ", "tags": "ì¶œê·¼ì¡°,ê¸°ê¸°ê³ ì¥"},
        "cancel": {"content": "ê³ ê°ì´ ìŒë£Œ ì£¼ë¬¸ì„ ì·¨ì†Œí•˜ê² ë‹¤ê³  í•¨", "tags": "ê³ ê°í´ë ˆì„,ì£¼ë¬¸ê´€ë¦¬"},
        "newbie": {"content": "ì‹ ì… ì§ì›ì´ ê³„ì‚°ì„ í‹€ë ¤ì„œ ë‹¹í™©í•¨", "tags": "ì‹ ì…êµìœ¡,ì‹¤ìˆ˜ì²˜ë¦¬"},
        "waiting": {"content": "ë§¤ì¥ì— ê³ ê°ì´ ì¤„ì„ ì„œì„œ ëŒ€ê¸° ì¤‘", "tags": "í˜¼ì¡ìƒí™©,ëŒ€ê¸°ê´€ë¦¬"},
        "delivery": {"content": "ë°°ë‹¬ ì£¼ë¬¸ì´ 5ê±´ ë™ì‹œì— ë“¤ì–´ì˜´", "tags": "ë°°ë‹¬,ë‹¤ì¤‘ì—…ë¬´"}
    }
    
    patterns = list(scenario_patterns.keys())
    pattern_index = (scenario_id % len(patterns))
    pattern_key = patterns[pattern_index]
    
    return scenario_patterns[pattern_key]

def build_scenarios_info_from_request(request: EducationalAnalysisRequest) -> str:
    """ìš”ì²­ì—ì„œ ë°›ì€ ì‹œë‚˜ë¦¬ì˜¤ IDë“¤ì„ ê¸°ë°˜ìœ¼ë¡œ 1~5 ìˆœì„œì˜ ì‹œë‚˜ë¦¬ì˜¤ ì •ë³´ ìƒì„±"""
    scenarios_info = ""
    
    for i, scenario_id in enumerate(request.userorder, 1):
        # âœ… ìˆ˜ì •: requestë¥¼ í•¨ê»˜ ì „ë‹¬
        scenario_info = get_scenario_info_by_id(scenario_id, request)
        scenarios_info += f"{i}. {scenario_info['content']} ({scenario_info['tags']})\n"
    
    return scenarios_info

# íŒíŠ¸ ìƒì„± ì—”ë“œí¬ì¸íŠ¸
@router.post("/get-hints")
async def get_hints_only(request: HintsRequest):
    """í˜ì´ì§€ ë¡œë”© ì‹œ ê° ìƒí™©ë³„ ì‘ëŒ€ íŒíŠ¸ë§Œ ìƒì„±"""
    
    if not client:
        print("OpenAI í´ë¼ì´ì–¸íŠ¸ê°€ ì„¤ì •ë˜ì§€ ì•ŠìŒ - ê¸°ë³¸ íŒíŠ¸ ë°˜í™˜")
        return create_default_hints(request.scenarios)
    
    scenarios = request.scenarios
    task = request.task
    
    print(f"íŒíŠ¸ ìš”ì²­ ë°›ìŒ: {len(scenarios)}ê°œ ì‹œë‚˜ë¦¬ì˜¤")
    
    if not scenarios:
        raise HTTPException(status_code=400, detail="ì‹œë‚˜ë¦¬ì˜¤ê°€ ì œê³µë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    
    if task != "generate_hints_only":
        raise HTTPException(status_code=400, detail="ì˜ëª»ëœ ì‘ì—… ìœ í˜•ì…ë‹ˆë‹¤.")
    
    try:
        prompt = generate_hints_only_prompt(scenarios)
        
        response = client.chat.completions.create(
            model="gpt-4",
            messages=[{"role": "user", "content": prompt}],
            temperature=0.7
        )
        
        gpt_response = response.choices[0].message.content
        print(f"GPT íŒíŠ¸ ì‘ë‹µ: {gpt_response}")
        
        hints_result = json.loads(gpt_response)
        return hints_result
        
    except OpenAIError as e:
        print(f"âŒ OpenAI API ì˜¤ë¥˜: {e}")
        return create_default_hints(scenarios)
        
    except json.JSONDecodeError as e:
        print(f"âŒ JSON íŒŒì‹± ì˜¤ë¥˜: {e}")
        print(f"íŒŒì‹± ì‹¤íŒ¨í•œ ì‘ë‹µ: {gpt_response}")
        return create_default_hints(scenarios)
        
    except Exception as e:
        print(f"âŒ íŒíŠ¸ ìƒì„± ì˜¤ë¥˜: {e}")
        return create_default_hints(scenarios)

# êµìœ¡ìš© ë¶„ì„ ì—”ë“œí¬ì¸íŠ¸
@router.post("/educational-analysis")
async def educational_analysis(request: EducationalAnalysisRequest):
    """êµìœ¡ ì¤‘ì‹¬ ì‹œë®¬ë ˆì´ì…˜ ë¶„ì„ - ì‹¤ì œ ì‹œë‚˜ë¦¬ì˜¤ ID ì§€ì›"""
    
    if not client:
        print("âŒ OpenAI í´ë¼ì´ì–¸íŠ¸ê°€ ì„¤ì •ë˜ì§€ ì•ŠìŒ")
        return create_default_educational_analysis(request)
    
    print(f"êµìœ¡ìš© ë¶„ì„ ìš”ì²­ - ì‚¬ìš©ì: {request.userid}")
    print(f"ì„ íƒí•œ ìˆœì„œ (ì‹¤ì œ ID): {request.userorder}")
    print(f"ì‹œë‚˜ë¦¬ì˜¤ ì •ë³´: {request.scenarioContents}")
    print(f"ì‚¬ìš©ì ì´ìœ : {request.reason}")
    
    # ë¬´ì˜ë¯¸í•œ ì…ë ¥ ê²€ì¦
    invalid_responses = get_invalid_responses(request)
    if invalid_responses:
        print(f"âš ï¸ ë¬´ì˜ë¯¸í•œ ì…ë ¥ ê°ì§€ - ì‹œë‚˜ë¦¬ì˜¤: {invalid_responses}")
    
    try:
        # 1ë‹¨ê³„: ì‚¬ìš©ìê°€ ì„ íƒí•œ ìˆœì„œë¥¼ ê¸°ë°˜ìœ¼ë¡œ 1~5 ì‹œë‚˜ë¦¬ì˜¤ ì •ë³´ ìƒì„±
        print("ğŸ”„ ì‹œë‚˜ë¦¬ì˜¤ ì •ë³´ êµ¬ì„± ì¤‘...")
        scenarios_info = build_scenarios_info_from_request(request)
        print(f"ìƒì„±ëœ ì‹œë‚˜ë¦¬ì˜¤ ì •ë³´:\n{scenarios_info}")
        
        # 2ë‹¨ê³„: GPT ì¶”ì²œ ìˆœì„œ ë° ì´ìœ  ìƒì„± (1~5 ê¸°ì¤€)
        print("ğŸ”„ GPT ì¶”ì²œ ìˆœì„œ ìƒì„± ì¤‘...")
        gpt_recommendation = await get_gpt_recommended_order_detailed(scenarios_info)
        
        # 3ë‹¨ê³„: êµìœ¡ìš© ë¶„ì„ í”„ë¡¬í”„íŠ¸ ìƒì„±
        print("ğŸ”„ êµìœ¡ìš© ë¶„ì„ í”„ë¡¬í”„íŠ¸ ìƒì„± ì¤‘...")
        educational_prompt = generate_educational_analysis_prompt(request, gpt_recommendation, invalid_responses)
        
        # GPT API í˜¸ì¶œ
        print("ğŸ”„ GPT API í˜¸ì¶œ ì¤‘...")
        response = client.chat.completions.create(
            model="gpt-4",
            messages=[{"role": "user", "content": prompt}],
            temperature=0.7,
            max_tokens=4000
        )
        
        gpt_response = response.choices[0].message.content
        print(f"âœ… GPT êµìœ¡ ë¶„ì„ ì‘ë‹µ ê¸¸ì´: {len(gpt_response)}")
        print(f"GPT ì‘ë‹µ ë¯¸ë¦¬ë³´ê¸°: {gpt_response[:200]}...")
        
        # JSON íŒŒì‹± ì‹œë„
        try:
            analysis_result = json.loads(gpt_response)
            print("âœ… JSON íŒŒì‹± ì„±ê³µ")
        except json.JSONDecodeError:
            # JSON ì¶”ì¶œ ì‹œë„
            print("âš ï¸ ì§ì ‘ JSON íŒŒì‹± ì‹¤íŒ¨, JSON ë¸”ë¡ ì¶”ì¶œ ì‹œë„...")
            json_match = re.search(r'```json\s*(\{.*?\})\s*```', gpt_response, re.DOTALL)
            if json_match:
                json_str = json_match.group(1)
                analysis_result = json.loads(json_str)
                print("âœ… JSON ë¸”ë¡ ì¶”ì¶œ ë° íŒŒì‹± ì„±ê³µ")
            else:
                print("âŒ JSON ë¸”ë¡ ì¶”ì¶œ ì‹¤íŒ¨, ê¸°ë³¸ê°’ ì‚¬ìš©")
                return create_improved_default_educational_analysis(request, invalid_responses)
        
        # ì‚¬ìš©ì ìˆœì„œ ì •ë³´ ì¶”ê°€ (ì‹¤ì œ ì‹œë‚˜ë¦¬ì˜¤ ID ì‚¬ìš©)
        analysis_result["userOrder"] = format_user_order_with_real_ids(request.userorder, request)
        
        return analysis_result
        
    except OpenAIError as e:
        print(f"âŒ OpenAI API ì˜¤ë¥˜: {e}")
        return create_improved_default_educational_analysis(request, invalid_responses)
        
    except json.JSONDecodeError as e:
        print(f"âŒ JSON íŒŒì‹± ì˜¤ë¥˜: {e}")
        print(f"íŒŒì‹± ì‹¤íŒ¨í•œ ì‘ë‹µ: {gpt_response if 'gpt_response' in locals() else 'N/A'}")
        return create_improved_default_educational_analysis(request, invalid_responses)
        
    except Exception as e:
        print(f"âŒ êµìœ¡ ë¶„ì„ ì˜¤ë¥˜: {e}")
        return create_improved_default_educational_analysis(request, invalid_responses)

# íŒíŠ¸ ê´€ë ¨ í•¨ìˆ˜ë“¤
def generate_hints_only_prompt(scenarios):
    """íŒíŠ¸ ì „ìš© í”„ë¡¬í”„íŠ¸ ìƒì„±"""
    
    prompt = """ë‹¤ìŒì€ ì¹´í˜ì—ì„œ ë°œìƒí•  ìˆ˜ ìˆëŠ” ìƒí™©ë“¤ì…ë‹ˆë‹¤. ê° ìƒí™©ì— ëŒ€í•œ ì‹¤ë¬´ì—ì„œ ë°”ë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” ê³ ê° ì‘ëŒ€ íŒíŠ¸ë¥¼ ì œê³µí•´ì£¼ì„¸ìš”.

ã€ìƒí™© ëª©ë¡ã€‘
"""
    
    for i, scenario in enumerate(scenarios):
        scenario_id = scenario.get('scenarioId')
        content = scenario.get('scenarioContent', '')
        tags = scenario.get('scenarioTag', '')
        prompt += f"{scenario_id}. {content} (íƒœê·¸: {tags})\n"
    
    prompt += """
ê° ìƒí™©ë³„ë¡œ í˜„ì‹¤ì ì´ê³  ì‹¤ìš©ì ì¸ ê³ ê° ì‘ëŒ€ íŒíŠ¸ë¥¼ ì œê³µí•´ì£¼ì„¸ìš”.
íƒœê·¸ì— ë§ëŠ” ìƒí™©ì„ ê³ ë ¤í•˜ì—¬ êµ¬ì²´ì ì¸ ë©˜íŠ¸ë‚˜ í–‰ë™ ì§€ì¹¨ì„ í¬í•¨í•´ì£¼ì„¸ìš”.

ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•´ì£¼ì„¸ìš”:
{
    "responseHints": {
"""
     
    for i, scenario in enumerate(scenarios):
        scenario_id = scenario.get('scenarioId')
        if i == len(scenarios) - 1:
            prompt += f'        "{scenario_id}": "ì´ ìƒí™©ì— ëŒ€í•œ êµ¬ì²´ì ì¸ ì‘ëŒ€ íŒíŠ¸"\n'
        else:
            prompt += f'        "{scenario_id}": "ì´ ìƒí™©ì— ëŒ€í•œ êµ¬ì²´ì ì¸ ì‘ëŒ€ íŒíŠ¸",\n'
    
    prompt += """    }
}

ë°˜ë“œì‹œ ìœ„ì˜ JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•˜ê³ , ë‹¤ë¥¸ ì„¤ëª…ì´ë‚˜ í…ìŠ¤íŠ¸ëŠ” í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”.
ê° íŒíŠ¸ëŠ” ì‹¤ë¬´ì—ì„œ ë°”ë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡ êµ¬ì²´ì ì´ê³  ì‹¤ìš©ì ìœ¼ë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”."""

    return prompt

def create_default_hints(scenarios):
    """GPT í˜¸ì¶œ ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ íŒíŠ¸ ë°˜í™˜"""
    default_hints = {}
    
    for scenario in scenarios:
        scenario_id = str(scenario.get('scenarioId'))
        content = scenario.get('scenarioContent', '')
        
        if 'ì»¤í”¼ë¨¸ì‹ ' in content or 'ê¸°ê¸°' in content:
            default_hints[scenario_id] = "ê¸°ê¸° ìƒíƒœë¥¼ í™•ì¸í•˜ê³ , í•„ìš”ì‹œ ì „ë¬¸ê°€ì—ê²Œ ì—°ë½í•˜ì„¸ìš”."
        elif 'ê³ ê°' in content and ('í´ë ˆì„' in content or 'ì·¨ì†Œ' in content):
            default_hints[scenario_id] = "ê³ ê°ì˜ ë§ì”€ì„ ì£¼ì˜ê¹Šê²Œ ë“£ê³ , ì§„ì‹¬ìœ¼ë¡œ ì‚¬ê³¼ë“œë¦¬ë©° í•´ê²°ë°©ì•ˆì„ ì œì‹œí•˜ì„¸ìš”."
        elif 'ì‹ ì…' in content or 'ê³„ì‚°' in content:
            default_hints[scenario_id] = "ì¹œì ˆí•˜ê²Œ ì •í™•í•œ ë°©ë²•ì„ ì•Œë ¤ì£¼ê³ , ì‹¤ìˆ˜ë¥¼ ê²©ë ¤ì˜ ê¸°íšŒë¡œ ë§Œë“œì„¸ìš”."
        elif 'ëŒ€ê¸°' in content or 'ì¤„' in content:
            default_hints[scenario_id] = "ê³ ê°ì—ê²Œ ìƒí™©ì„ ì„¤ëª…í•˜ê³ , ëŒ€ê¸°ì‹œê°„ì„ ìµœì†Œí™”í•˜ë„ë¡ ë…¸ë ¥í•˜ì„¸ìš”."
        elif 'ë°°ë‹¬' in content:
            default_hints[scenario_id] = "ì£¼ë¬¸ì„ ì •í™•íˆ í™•ì¸í•˜ê³ , íš¨ìœ¨ì ì¸ ìˆœì„œë¡œ ì¤€ë¹„í•˜ì„¸ìš”."
        else:
            default_hints[scenario_id] = "ìƒí™©ì— ë§ëŠ” ì ì ˆí•œ ëŒ€ì‘ì„ í•˜ì„¸ìš”."
    
    return {"responseHints": default_hints}

# êµìœ¡ ë¶„ì„ ê´€ë ¨ í•¨ìˆ˜ë“¤
async def get_gpt_recommended_order_detailed(scenarios_info: str):
    """GPTê°€ ì¶”ì²œí•˜ëŠ” ì²˜ë¦¬ ìˆœì„œ ë° ìƒì„¸ ì´ìœ  - 1~5 ê¸°ì¤€ ì‚¬ìš©"""
    
    prompt = f"""
    ì¹´í˜ì—ì„œ ë‹¤ìŒ 5ê°€ì§€ ìƒí™©ì´ ë™ì‹œì— ë°œìƒí–ˆìŠµë‹ˆë‹¤:
    {scenarios_info}
    
    ì „ë¬¸ì ì¸ ì¹´í˜ ë§¤ë‹ˆì € ê´€ì ì—ì„œ ì´ ìƒí™©ë“¤ì„ ì²˜ë¦¬í•  ìµœì ì˜ ìš°ì„ ìˆœìœ„ë¥¼ ì •í•˜ê³ , 
    ìš°ì„ ìˆœìœ„ íŒë‹¨ ê¸°ì¤€ê³¼ ìƒì„¸í•œ ì´ìœ ë¥¼ ì„¤ëª…í•´ì£¼ì„¸ìš”.
    
    **ì¤‘ìš”: ì¶”ì²œ ìˆœì„œëŠ” ë°˜ë“œì‹œ [1, 2, 3, 4, 5] ìˆ«ìë¡œ ë°°ì—´í•´ì£¼ì„¸ìš”.**
    (1ë²ˆì´ ì²« ë²ˆì§¸ ìƒí™©, 2ë²ˆì´ ë‘ ë²ˆì§¸ ìƒí™©, 3ë²ˆì´ ì„¸ ë²ˆì§¸ ìƒí™©, 4ë²ˆì´ ë„¤ ë²ˆì§¸ ìƒí™©, 5ë²ˆì´ ë‹¤ì„¯ ë²ˆì§¸ ìƒí™©)
    
    ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•´ì£¼ì„¸ìš”:
    {{
        "recommendedOrder": [1, 2, 3, 4, 5 ì¤‘ ìˆœì„œ ë°°ì—´],
        "priorityCriteria": "ìš°ì„ ìˆœìœ„ë¥¼ ì •í•˜ëŠ” íŒë‹¨ ê¸°ì¤€ (1-2ë¬¸ì¥)",
        "detailedReasoning": "ìš°ì„ ìˆœìœ„ë¥¼ ì´ë ‡ê²Œ ì •í•œ ìƒì„¸í•œ ì´ìœ "
    }}
    """
    
    try:
        if not client:
            raise Exception("OpenAI í´ë¼ì´ì–¸íŠ¸ê°€ ì—†ìŒ")
            
        response = client.chat.completions.create(
            model="gpt-4",
            messages=[{"role": "user", "content": prompt}],
            temperature=0.7
        )
        
        gpt_response = response.choices[0].message.content
        return json.loads(gpt_response)
        
    except Exception as e:
        print(f"âŒ GPT ì¶”ì²œ ìˆœì„œ ìƒì„± ì‹¤íŒ¨: {e}")
        return {
            "recommendedOrder": [1, 4, 2, 5, 3],
            "priorityCriteria": "ë§¤ì¥ ìš´ì˜ì— ë¯¸ì¹˜ëŠ” íŒŒê¸‰íš¨ê³¼ì™€ ê³ ê°ì´ ì§ì ‘ ì²´ê°í•˜ëŠ” ë¶ˆí¸í•¨ì˜ ì‹œê¸‰ì„±ì„ ì¢…í•©ì ìœ¼ë¡œ ê³ ë ¤í•©ë‹ˆë‹¤.",
            "detailedReasoning": "ì»¤í”¼ë¨¸ì‹  ê³ ì¥ì€ ë§¤ì¥ì˜ í•µì‹¬ ê¸°ëŠ¥ì— ì˜í–¥ì„ ë¯¸ì¹˜ë¯€ë¡œ ìµœìš°ì„ ìœ¼ë¡œ ì²˜ë¦¬í•´ì•¼ í•©ë‹ˆë‹¤. ëŒ€ê¸° ê³ ê°ì€ ì¦‰ì‹œ ë³´ì´ëŠ” ë¬¸ì œì´ë¯€ë¡œ ë‘ ë²ˆì§¸ë¡œ ì²˜ë¦¬í•˜ê³ , ì£¼ë¬¸ ì·¨ì†ŒëŠ” ê°œë³„ ê³ ê° ë§Œì¡±ë„ì— ì§ì ‘ ì˜í–¥ì„ ë¯¸ì¹˜ë¯€ë¡œ ì„¸ ë²ˆì§¸ì…ë‹ˆë‹¤. ë°°ë‹¬ ì£¼ë¬¸ì€ ì—¬ëŸ¬ ê³ ê°ì´ ê¸°ë‹¤ë¦¬ê³  ìˆì–´ ë„¤ ë²ˆì§¸ë¡œ ì²˜ë¦¬í•˜ë©°, ì‹ ì… ì§ì› êµìœ¡ì€ ì¥ê¸°ì  ê´€ì ì—ì„œ ì¤‘ìš”í•˜ì§€ë§Œ ì¦‰ì‹œì„±ì´ ìƒëŒ€ì ìœ¼ë¡œ ë‚®ì•„ ë§ˆì§€ë§‰ìœ¼ë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤."
        }

def analyze_time_data(request: EducationalAnalysisRequest) -> str:
    """ì‹œê°„ ë°ì´í„° ë¶„ì„ - 5ì´ˆ ì´í•˜ì¼ ë•Œë§Œ ì°¸ì—¬ë„ ë¶€ì¡± íŒì •"""
    order_time = request.orderSelectionTime
    reason_time = request.reasonWritingTime
    total_time = request.totalTimeSpent
    
    analysis = f"ìˆœì„œ ì„ íƒ ì‹œê°„: {order_time}ì´ˆ, ì´ìœ  ì‘ì„± ì‹œê°„: {reason_time}ì´ˆ, ì „ì²´ ì†Œìš” ì‹œê°„: {total_time}ì´ˆ\n"
    
    # ìˆœì„œ ì„ íƒ ì‹œê°„ ë¶„ì„ (5ì´ˆ ì´í•˜ë§Œ ë¶€ì¡± íŒì •)
    if order_time <= 5:  # 5ì´ˆ ì´í•˜
        analysis += "ìˆœì„œ ì„ íƒì´ ë„ˆë¬´ ë¹¨ëìŠµë‹ˆë‹¤. ì¡°ê¸ˆ ë” ì‹ ì¤‘í•˜ê²Œ ê³ ë¯¼í•´ë³´ì‹œë©´ ì¢‹ê² ì–´ìš”. "
    elif order_time <= 30:  # 30ì´ˆ ì´í•˜
        analysis += "ìˆœì„œ ì„ íƒì„ ë¹ ë¥´ê²Œ ì™„ë£Œí•˜ì…¨ë„¤ìš”. "
    elif order_time <= 90:  # 1ë¶„ 30ì´ˆ ì´í•˜
        analysis += "ìˆœì„œ ì„ íƒì„ ì ì ˆí•œ ì‹œê°„ ë‚´ì— ì™„ë£Œí•˜ì…¨ìŠµë‹ˆë‹¤. "
    elif order_time <= 180:  # 3ë¶„ ì´í•˜
        analysis += "ìˆœì„œ ì„ íƒì— ì¶©ë¶„í•œ ì‹œê°„ì„ íˆ¬ìí•˜ì…¨ìŠµë‹ˆë‹¤. "
    else:  # 3ë¶„ ì´ˆê³¼
        analysis += "ìˆœì„œ ì„ íƒì„ ë§¤ìš° ì‹ ì¤‘í•˜ê²Œ í•˜ì…¨ë„¤ìš”. "
    
    # ì´ìœ  ì‘ì„± ì‹œê°„ ë¶„ì„ (5ì´ˆ ì´í•˜ë§Œ ë¶€ì¡± íŒì •)
    if reason_time <= 5:  # 5ì´ˆ ì´í•˜
        analysis += "ì´ìœ  ì‘ì„±ì´ ë„ˆë¬´ ë¹¨ëìŠµë‹ˆë‹¤. ì¡°ê¸ˆ ë” ìì„¸í•˜ê²Œ ìƒê°í•´ë³´ì‹œë©´ ì¢‹ê² ì–´ìš”. "
    elif reason_time <= 60:  # 1ë¶„ ì´í•˜
        analysis += "ì´ìœ  ì‘ì„±ì„ ë¹ ë¥´ê²Œ ì™„ë£Œí•˜ì…¨ë„¤ìš”. "
    elif reason_time <= 120:  # 2ë¶„ ì´í•˜
        analysis += "ì´ìœ  ì‘ì„±ì— ì ì ˆí•œ ì‹œê°„ì„ ì‚¬ìš©í•˜ì…¨ìŠµë‹ˆë‹¤. "
    elif reason_time <= 300:  # 5ë¶„ ì´í•˜
        analysis += "ì´ìœ  ì‘ì„±ì— ì¶©ë¶„í•œ ì‹œê°„ì„ íˆ¬ìí•˜ì…¨ìŠµë‹ˆë‹¤. "
    else:  # 5ë¶„ ì´ˆê³¼
        analysis += "ì´ìœ  ì‘ì„±ì„ ë§¤ìš° ê¼¼ê¼¼í•˜ê²Œ í•˜ì…¨ë„¤ìš”. "
    
    return analysis

def analyze_text_quality(request: EducationalAnalysisRequest, invalid_responses: List[str]) -> str:
    """í…ìŠ¤íŠ¸ í’ˆì§ˆ ë¶„ì„ - 5ì ì´í•˜ì¼ ë•Œë§Œ ì°¸ì—¬ë„ ë¶€ì¡± íŒì •"""
    reason_length = len(request.reason.strip())
    
    total_response_length = sum(len(text.strip()) for text in request.responseTexts.values())
    avg_response_length = total_response_length / len(request.responseTexts) if request.responseTexts else 0
    
    analysis = f"ì´ìœ  ì‘ì„± ê¸¸ì´: {reason_length}ì, í‰ê·  ì‘ëŒ€ ë©˜íŠ¸ ê¸¸ì´: {avg_response_length:.1f}ì\n"
    
    # ë¬´ì˜ë¯¸í•œ ì…ë ¥ ì²´í¬
    if invalid_responses:
        analysis += f"ì‹œë‚˜ë¦¬ì˜¤ {', '.join(invalid_responses)}ì˜ ë©˜íŠ¸ê°€ ë¬´ì˜ë¯¸í•œ ì…ë ¥ìœ¼ë¡œ ë³´ì…ë‹ˆë‹¤. "
    
    # ì´ìœ  ì‘ì„± ê¸¸ì´ ë¶„ì„ (5ì ì´í•˜ë§Œ ë¶€ì¡± íŒì •)
    if reason_length <= 5:  # 5ì ì´í•˜
        analysis += "ìˆœì„œ ì„ íƒ ì´ìœ ê°€ ë„ˆë¬´ ê°„ë‹¨í•©ë‹ˆë‹¤. ì¡°ê¸ˆ ë” ìì„¸í•œ ì„¤ëª…ì„ ì¶”ê°€í•´ì£¼ì‹œë©´ ì¢‹ê² ì–´ìš”. "
    elif reason_length <= 30:  # 30ì ì´í•˜
        analysis += "ìˆœì„œ ì„ íƒ ì´ìœ ë¥¼ ê°„ë‹¨í•˜ê²Œ ì‘ì„±í•˜ì…¨ë„¤ìš”. "
    elif reason_length <= 100:  # 100ì ì´í•˜
        analysis += "ìˆœì„œ ì„ íƒ ì´ìœ ë¥¼ ì ì ˆí•˜ê²Œ ì‘ì„±í•˜ì…¨ìŠµë‹ˆë‹¤. "
    elif reason_length <= 300:  # 300ì ì´í•˜
        analysis += "ìˆœì„œ ì„ íƒ ì´ìœ ë¥¼ ìƒì„¸í•˜ê²Œ ì‘ì„±í•˜ì…¨ë„¤ìš”. "
    else:  # 300ì ì´ˆê³¼
        analysis += "ìˆœì„œ ì„ íƒ ì´ìœ ë¥¼ ë§¤ìš° ìì„¸í•˜ê²Œ ì‘ì„±í•´ì£¼ì…¨ìŠµë‹ˆë‹¤. "
    
    # ì‘ëŒ€ ë©˜íŠ¸ ê¸¸ì´ ë¶„ì„ (5ì ì´í•˜ë§Œ ë¶€ì¡± íŒì •)
    if avg_response_length <= 5:  # 5ì ì´í•˜ (ì•ˆë…•í•˜ì„¸ìš” ìˆ˜ì¤€)
        analysis += "ì‘ëŒ€ ë©˜íŠ¸ê°€ ë„ˆë¬´ ê°„ë‹¨í•©ë‹ˆë‹¤. ì¢€ ë” êµ¬ì²´ì ì¸ í‘œí˜„ì„ ì¶”ê°€í•´ì£¼ì‹œë©´ ì¢‹ê² ì–´ìš”. "
    elif avg_response_length <= 20:  # 20ì ì´í•˜
        analysis += "ì‘ëŒ€ ë©˜íŠ¸ë¥¼ ê°„ë‹¨í•˜ê²Œ ì‘ì„±í•˜ì…¨ë„¤ìš”. "
    elif avg_response_length <= 50:  # 50ì ì´í•˜
        analysis += "ì‘ëŒ€ ë©˜íŠ¸ë¥¼ ì ì ˆí•˜ê²Œ ì‘ì„±í•˜ì…¨ìŠµë‹ˆë‹¤. "
    elif avg_response_length <= 100:  # 100ì ì´í•˜
        analysis += "ì‘ëŒ€ ë©˜íŠ¸ë¥¼ ìƒì„¸í•˜ê²Œ ì‘ì„±í•˜ì…¨ë„¤ìš”. "
    else:  # 100ì ì´ˆê³¼
        analysis += "ì‘ëŒ€ ë©˜íŠ¸ë¥¼ ë§¤ìš° êµ¬ì²´ì ìœ¼ë¡œ ì‘ì„±í•´ì£¼ì…¨ìŠµë‹ˆë‹¤. "
    
    return analysis

def generate_educational_analysis_prompt(request: EducationalAnalysisRequest, gpt_recommendation: dict, invalid_responses: List[str]):
    """êµìœ¡ìš© ë¶„ì„ í”„ë¡¬í”„íŠ¸ ìƒì„± - ì‹¤ì œ ì‹œë‚˜ë¦¬ì˜¤ ID ì§€ì›"""
    
    # ì‹œê°„ ë¶„ì„
    time_analysis = analyze_time_data(request)
    
    # í…ìŠ¤íŠ¸ ê¸¸ì´ ë¶„ì„
    text_analysis = analyze_text_quality(request, invalid_responses)
    
    # ì‚¬ìš©ìê°€ ì„ íƒí•œ ìˆœì„œë¥¼ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜ (1~5 ìˆœì„œ ê¸°ì¤€)
    user_order_text = ""
    for i, scenario_id in enumerate(request.userorder):
        # âœ… ìˆ˜ì •: requestë¥¼ í•¨ê»˜ ì „ë‹¬
        scenario_info = get_scenario_info_by_id(scenario_id, request)
        user_order_text += f"{i+1}ìˆœìœ„: {scenario_info['content']} (ID: {scenario_id})\n"
    
    # GPT ì¶”ì²œ ìˆœì„œë¥¼ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜ (1~5 ìˆœì„œ ê¸°ì¤€)
    gpt_order_text = ""
    for i, position in enumerate(gpt_recommendation.get('recommendedOrder', [])):
        # positionì€ 1~5 ì¤‘ í•˜ë‚˜, ì´ëŠ” ì‚¬ìš©ì ì„ íƒ ìˆœì„œì˜ ì¸ë±ìŠ¤ë¥¼ ì˜ë¯¸
        if position <= len(request.userorder):
            actual_scenario_id = request.userorder[position - 1]
            # âœ… ìˆ˜ì •: requestë¥¼ í•¨ê»˜ ì „ë‹¬
            scenario_info = get_scenario_info_by_id(actual_scenario_id, request)
            gpt_order_text += f"{i+1}ìˆœìœ„: {scenario_info['content']} (ID: {actual_scenario_id})\n"
    
    # ê° ì‹œë‚˜ë¦¬ì˜¤ë³„ ë©˜íŠ¸ (ì‹¤ì œ ID ê¸°ì¤€)
    response_texts = ""
    for i, scenario_id in enumerate(request.userorder, 1):
        # âœ… ìˆ˜ì •: requestë¥¼ í•¨ê»˜ ì „ë‹¬
        scenario_info = get_scenario_info_by_id(scenario_id, request)
        response_text = request.responseTexts.get(str(scenario_id), "")
        
        # ë¬´ì˜ë¯¸í•œ ì…ë ¥ì¸ì§€ ì²´í¬
        is_invalid = str(scenario_id) in invalid_responses
        invalid_note = " [âš ï¸ ë¬´ì˜ë¯¸í•œ ì…ë ¥]" if is_invalid else ""
        
        response_texts += f"ì‹œë‚˜ë¦¬ì˜¤ {i} (ID: {scenario_id}) - {scenario_info['content']}{invalid_note}\nì‚¬ìš©ì ë©˜íŠ¸: '{response_text}'\n\n"
    
    # ë¬´ì˜ë¯¸í•œ ì…ë ¥ì— ëŒ€í•œ íŠ¹ë³„ ì§€ì¹¨
    invalid_guidance = ""
    if invalid_responses:
        invalid_guidance = f"""
    
    ã€âš ï¸ ë¬´ì˜ë¯¸í•œ ì…ë ¥ ê°ì§€ã€‘
    ì‹œë‚˜ë¦¬ì˜¤ {', '.join(invalid_responses)}ì—ì„œ ë¬´ì˜ë¯¸í•œ ì…ë ¥ì´ ê°ì§€ë˜ì—ˆìŠµë‹ˆë‹¤.
    í•´ë‹¹ ì‹œë‚˜ë¦¬ì˜¤ë“¤ì— ëŒ€í•´ì„œëŠ” "ì´ ì‹œë‚˜ë¦¬ì˜¤ì— ëŒ€í•´ ì˜ë¯¸ìˆëŠ” ì‘ë‹µì„ ì‘ì„±í•´ì£¼ì‹œì§€ ì•Šìœ¼ì…¨ë„¤ìš”. ì‹¤ì œ ê³ ê° ìƒí™©ì—ì„œ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” êµ¬ì²´ì ì¸ ë©˜íŠ¸ë¥¼ ì‘ì„±í•´ë³´ì‹œë©´ ì–´ë–¨ê¹Œìš”?"ë¼ëŠ” ì•ˆë‚´ë¥¼ í¬í•¨í•´ì£¼ì„¸ìš”.
    """
    
    prompt = f"""
    ì¹´í˜ ì‹œë®¬ë ˆì´ì…˜ êµìœ¡ìš© ë¶„ì„ì„ ìˆ˜í–‰í•´ì£¼ì„¸ìš”. ì ìˆ˜ë‚˜ ë“±ê¸‰ ì—†ì´ ìˆœìˆ˜ êµìœ¡ì  ê´€ì ì—ì„œ í”¼ë“œë°±í•´ì£¼ì„¸ìš”.

    ã€GPT ì¶”ì²œ ìˆœì„œã€‘
    {gpt_order_text}
    íŒë‹¨ ê¸°ì¤€: {gpt_recommendation.get('priorityCriteria', '')}

    ã€ì‚¬ìš©ì ì„ íƒ ìˆœì„œã€‘
    {user_order_text}
    ì„ íƒ ì´ìœ : {request.reason}

    ã€ì‚¬ìš©ì ì‘ëŒ€ ë©˜íŠ¸ã€‘
    {response_texts}

    ã€ì‹œê°„ ë° ì°¸ì—¬ë„ ë¶„ì„ã€‘
    {time_analysis}
    {text_analysis}
    {invalid_guidance}

    ì‚¬ìš©ìê°€ ì‹¤ì œë¡œ ì‘ì„±í•œ ë©˜íŠ¸ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ê°œë³„ì ì´ê³  êµ¬ì²´ì ì¸ ì½”ì¹­ì„ ì œê³µí•´ì£¼ì„¸ìš”.
    ê° ì‹œë‚˜ë¦¬ì˜¤ë³„ë¡œ ì‚¬ìš©ìì˜ ì‹¤ì œ ë©˜íŠ¸ ë‚´ìš©ì„ ë¶„ì„í•˜ì—¬ ë§ì¶¤í˜• ê°œì„ ì ì„ ì œì‹œí•´ì•¼ í•©ë‹ˆë‹¤.

    ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•´ì£¼ì„¸ìš”:
    {{
        "participationFeedback": "í•™ìŠµ ì°¸ì—¬ë„ì— ëŒ€í•œ í”¼ë“œë°± (ì‹œê°„ê³¼ í…ìŠ¤íŠ¸ ê¸¸ì´, ë¬´ì˜ë¯¸í•œ ì…ë ¥ ê³ ë ¤)",
        "scenarioCoaching": {{
            "scenario1": ["ì‚¬ìš©ì ë©˜íŠ¸ì— ëŒ€í•œ êµ¬ì²´ì  ê°œì„ ì  1", "ì‚¬ìš©ì ë©˜íŠ¸ì— ëŒ€í•œ êµ¬ì²´ì  ê°œì„ ì  2"],
            "scenario2": ["ì‚¬ìš©ì ë©˜íŠ¸ì— ëŒ€í•œ êµ¬ì²´ì  ê°œì„ ì  1", "ì‚¬ìš©ì ë©˜íŠ¸ì— ëŒ€í•œ êµ¬ì²´ì  ê°œì„ ì  2"],
            "scenario3": ["ì‚¬ìš©ì ë©˜íŠ¸ì— ëŒ€í•œ êµ¬ì²´ì  ê°œì„ ì  1", "ì‚¬ìš©ì ë©˜íŠ¸ì— ëŒ€í•œ êµ¬ì²´ì  ê°œì„ ì  2"],
            "scenario4": ["ì‚¬ìš©ì ë©˜íŠ¸ì— ëŒ€í•œ êµ¬ì²´ì  ê°œì„ ì  1", "ì‚¬ìš©ì ë©˜íŠ¸ì— ëŒ€í•œ êµ¬ì²´ì  ê°œì„ ì  2"],
            "scenario5": ["ì‚¬ìš©ì ë©˜íŠ¸ì— ëŒ€í•œ êµ¬ì²´ì  ê°œì„ ì  1", "ì‚¬ìš©ì ë©˜íŠ¸ì— ëŒ€í•œ êµ¬ì²´ì  ê°œì„ ì  2"]
        }},
        "orderAnalysis": "ì‚¬ìš©ì ìˆœì„œì— ëŒ€í•œ ë¶„ì„ê³¼ ê°œì„  ë°©í–¥ì„ ìì—°ìŠ¤ëŸ¬ìš´ ê¸€ë¡œ ì‘ì„±",
        "strengths": [
            "ê°•ì  1ì— ëŒ€í•œ ìƒì„¸ ì„¤ëª…",
            "ê°•ì  2ì— ëŒ€í•œ ìƒì„¸ ì„¤ëª…", 
            "ê°•ì  3ì— ëŒ€í•œ ìƒì„¸ ì„¤ëª…"
        ],
        "learningDirections": [
            "í•™ìŠµ ë°©í–¥ 1ì— ëŒ€í•œ êµ¬ì²´ì  ì„¤ëª…",
            "í•™ìŠµ ë°©í–¥ 2ì— ëŒ€í•œ êµ¬ì²´ì  ì„¤ëª…",
            "í•™ìŠµ ë°©í–¥ 3ì— ëŒ€í•œ êµ¬ì²´ì  ì„¤ëª…"
        ],
        "gptOrderDetails": {{
            "recommendedOrder": [1, 2, 3, 4, 5 ìˆœì„œ ë°°ì—´],
            "formattedOrderList": [
                "1ìˆœìœ„: ì‹œë‚˜ë¦¬ì˜¤ëª…",
                "2ìˆœìœ„: ì‹œë‚˜ë¦¬ì˜¤ëª…",
                "3ìˆœìœ„: ì‹œë‚˜ë¦¬ì˜¤ëª…",
                "4ìˆœìœ„: ì‹œë‚˜ë¦¬ì˜¤ëª…",
                "5ìˆœìœ„: ì‹œë‚˜ë¦¬ì˜¤ëª…"
            ]
        }},
        "gptReasoningDetails": {{
            "priorityCriteria": "GPTê°€ ìˆœì„œë¥¼ ì •í•œ íŒë‹¨ ê¸°ì¤€",
            "detailedReasoning": "GPTê°€ ì´ëŸ° ìˆœì„œë¥¼ ì¶”ì²œí•˜ëŠ” ìƒì„¸í•œ ì´ìœ ì™€ ê° ìˆœìœ„ë³„ íŒë‹¨ ê·¼ê±°"
        }}
    }}

    **ì¤‘ìš”**: ê° ì‹œë‚˜ë¦¬ì˜¤ ì½”ì¹­ì€ ë°˜ë“œì‹œ ì‚¬ìš©ìê°€ ì‹¤ì œë¡œ ì‘ì„±í•œ ë©˜íŠ¸ ë‚´ìš©ì„ ë¶„ì„í•˜ì—¬ ê°œë³„ì ìœ¼ë¡œ ì‘ì„±í•´ì•¼ í•©ë‹ˆë‹¤.
    ë™ì¼í•œ í”¼ë“œë°±ì„ ì—¬ëŸ¬ ì‹œë‚˜ë¦¬ì˜¤ì— ë°˜ë³µ ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”.
    ë¬´ì˜ë¯¸í•œ ì…ë ¥ì´ ê°ì§€ëœ ì‹œë‚˜ë¦¬ì˜¤ì—ëŠ” ì ì ˆí•œ ì•ˆë‚´ ë©”ì‹œì§€ë¥¼ í¬í•¨í•´ì£¼ì„¸ìš”.
    ì½”ì¹­ í†¤: "~í•˜ì‹œë©´ ë” ì¢‹ì„ ê²ƒ ê°™ì•„ìš”", "~í•´ë³´ì‹œëŠ” ê±´ ì–´ë–¨ê¹Œìš”" ê°™ì€ ë¶€ë“œëŸ½ê³  ì œì•ˆí•˜ëŠ” ë§íˆ¬
    """
    
    return prompt

def format_user_order_with_real_ids(userorder: List[int], request: EducationalAnalysisRequest = None) -> dict:
    """ì‹¤ì œ ì‹œë‚˜ë¦¬ì˜¤ IDë¥¼ ì‚¬ìš©í•œ ì‚¬ìš©ì ìˆœì„œ ì •ë³´ í¬ë§·íŒ…"""
    
    formatted_order = []
    for i, scenario_id in enumerate(userorder):
        # âœ… ìˆ˜ì •: requestë¥¼ í•¨ê»˜ ì „ë‹¬
        scenario_info = get_scenario_info_by_id(scenario_id, request)
        formatted_order.append({
            "priority": i + 1,
            "scenarioId": scenario_id,
            "scenarioName": scenario_info['content']
        })
    
    return {
        "order": userorder,
        "formattedOrder": formatted_order
    }

def create_improved_default_educational_analysis(request: EducationalAnalysisRequest, invalid_responses: List[str] = None):
    """ê°œì„ ëœ ê¸°ë³¸ êµìœ¡ ë¶„ì„ ê²°ê³¼ ë°˜í™˜ - ì‹¤ì œ ì‹œë‚˜ë¦¬ì˜¤ ID ì§€ì›"""
    
    if invalid_responses is None:
        invalid_responses = get_invalid_responses(request)
    
    print("âš ï¸ GPT ë¶„ì„ ì‹¤íŒ¨ - ê°œì„ ëœ ê¸°ë³¸ ë¶„ì„ ì‚¬ìš©")
    
    # ì°¸ì—¬ë„ ì²´í¬ (5ì´ˆ, 5ì ê¸°ì¤€)
    order_time = request.orderSelectionTime
    reason_time = request.reasonWritingTime
    reason_length = len(request.reason.strip())
    
    # ì°¸ì—¬ë„ í”¼ë“œë°± (5ì´ˆ, 5ì ì´í•˜ì¼ ë•Œë§Œ ë¶€ì¡± íŒì •)
    low_participation = (order_time <= 5 or reason_time <= 5 or reason_length <= 5)
    has_invalid_responses = len(invalid_responses) > 0
    
    if low_participation or has_invalid_responses:
        participation_feedback = "êµìœ¡ íš¨ê³¼ë¥¼ ë†’ì´ê¸° ìœ„í•´ì„œëŠ” ì¢€ ë” ì‹ ì¤‘í•˜ê²Œ ìƒê°í•´ë³´ì‹œê³  ìƒì„¸í•˜ê²Œ ì‘ì„±í•´ì£¼ì‹œë©´ ì¢‹ê² ì–´ìš”. "
        if has_invalid_responses:
            participation_feedback += f"íŠ¹íˆ ì‹œë‚˜ë¦¬ì˜¤ {', '.join(invalid_responses)}ì—ì„œëŠ” ì˜ë¯¸ìˆëŠ” ì‘ë‹µì„ ì‘ì„±í•´ì£¼ì‹œëŠ” ê²ƒì´ ì¤‘ìš”í•©ë‹ˆë‹¤. "
        participation_feedback += "ë” ê¹Šì´ ìˆëŠ” í•™ìŠµì„ ìœ„í•´ ë‹¤ìŒì—ëŠ” ì‹œê°„ì„ ì¶©ë¶„íˆ ê°€ì§€ê³  ì°¸ì—¬í•´ë³´ì„¸ìš”."
    else:
        participation_feedback = "êµìœ¡ì— ì„±ì‹¤í•˜ê²Œ ì°¸ì—¬í•´ì£¼ì…”ì„œ ê°ì‚¬í•©ë‹ˆë‹¤. ì´ëŸ° ìì„¸ë¡œ ê³„ì† í•™ìŠµí•´ë‚˜ê°€ì‹œë©´ ì¢‹ì€ ì„±ê³¼ë¥¼ ì–»ìœ¼ì‹¤ ìˆ˜ ìˆì„ ê±°ì˜ˆìš”."
    
    # ê°œë³„ ì‹œë‚˜ë¦¬ì˜¤ ì½”ì¹­ ìƒì„± (ì‹¤ì œ ì‹œë‚˜ë¦¬ì˜¤ ID ê¸°ë°˜)
    coaching = {}
    for i in range(1, 6):
        scenario_key = f"scenario{i}"
        
        # ì‹¤ì œ ì‹œë‚˜ë¦¬ì˜¤ ID ê°€ì ¸ì˜¤ê¸°
        if i <= len(request.userorder):
            actual_scenario_id = request.userorder[i-1]
            user_text = request.responseTexts.get(str(actual_scenario_id), "").strip()
        else:
            user_text = ""
        
        # ë¬´ì˜ë¯¸í•œ ì…ë ¥ì¸ì§€ ì²´í¬
        if str(actual_scenario_id) in invalid_responses:
            coaching[scenario_key] = [
                "ì‘ì„±í•´ì£¼ì‹  ë©˜íŠ¸ê°€ ì¢‹ì€ ë°©í–¥ì´ì§€ë§Œ, ì¢€ ë” êµ¬ì²´ì ìœ¼ë¡œ í‘œí˜„í•´ë³´ì‹œë©´ ì–´ë–¨ê¹Œìš”?",
                "ê³ ê°ì˜ ì…ì¥ì—ì„œ ìƒê°í•´ë³´ì‹œëŠ” ê²ƒë„ ë„ì›€ì´ ë  ê²ƒ ê°™ì•„ìš”"
            ]
        elif not user_text:
            coaching[scenario_key] = [
                "ì‘ì„±í•´ì£¼ì‹  ë©˜íŠ¸ê°€ ì¢‹ì€ ë°©í–¥ì´ì§€ë§Œ, ì¢€ ë” êµ¬ì²´ì ìœ¼ë¡œ í‘œí˜„í•´ë³´ì‹œë©´ ì–´ë–¨ê¹Œìš”?",
                "ê³ ê°ì˜ ì…ì¥ì—ì„œ ìƒê°í•´ë³´ì‹œëŠ” ê²ƒë„ ë„ì›€ì´ ë  ê²ƒ ê°™ì•„ìš”"
            ]
        elif len(user_text) <= 5:  # 5ì ì´í•˜ë§Œ ë¶€ì¡± íŒì •
            coaching[scenario_key] = [
                "ì‘ì„±í•´ì£¼ì‹  ë©˜íŠ¸ê°€ ì¢‹ì€ ë°©í–¥ì´ì§€ë§Œ, ì¢€ ë” êµ¬ì²´ì ìœ¼ë¡œ í‘œí˜„í•´ë³´ì‹œë©´ ì–´ë–¨ê¹Œìš”?",
                "ê³ ê°ì˜ ì…ì¥ì—ì„œ ìƒê°í•´ë³´ì‹œëŠ” ê²ƒë„ ë„ì›€ì´ ë  ê²ƒ ê°™ì•„ìš”"
            ]
        else:
            coaching[scenario_key] = [
                "ì‘ì„±í•´ì£¼ì‹  ë©˜íŠ¸ê°€ ì¢‹ì€ ë°©í–¥ì´ì§€ë§Œ, ì¢€ ë” êµ¬ì²´ì ìœ¼ë¡œ í‘œí˜„í•´ë³´ì‹œë©´ ì–´ë–¨ê¹Œìš”?",
                "ê³ ê°ì˜ ì…ì¥ì—ì„œ ìƒê°í•´ë³´ì‹œëŠ” ê²ƒë„ ë„ì›€ì´ ë  ê²ƒ ê°™ì•„ìš”"
            ]
    
    # ìˆœì„œ ë¶„ì„
    gpt_order = [1, 4, 2, 5, 3]  # ê¸°ë³¸ ì¶”ì²œ ìˆœì„œ (1~5 ê¸°ì¤€)
    
    # ì‚¬ìš©ì ìˆœì„œë¥¼ 1~5 ê¸°ì¤€ìœ¼ë¡œ ë³€í™˜í•˜ì—¬ ë¹„êµ
    user_order_normalized = list(range(1, len(request.userorder) + 1))
    
    order_analysis = "ê¸°ë³¸ì ì¸ ìƒí™© ì¸ì‹ ëŠ¥ë ¥ì„ ê°–ì¶”ê³  ê³„ì‹œì§€ë§Œ, ê³ ê° ì¤‘ì‹¬ì  ì‚¬ê³ ë¥¼ ì¡°ê¸ˆ ë” ë³´ê°•í•˜ì‹œë©´ ë”ìš± ê· í˜• ì¡íŒ íŒë‹¨ì„ í•˜ì‹¤ ìˆ˜ ìˆì„ ê²ƒ ê°™ì•„ìš”."
    
    # í•™ìŠµ ë°©í–¥
    learning_directions = [
        "ê³ ê° ì¤‘ì‹¬ ì‚¬ê³  ê¸°ë¥´ê¸°ë¥¼ ì—°ìŠµí•´ë³´ì„¸ìš”",
        "ìƒí™©ë³„ ì»¤ë®¤ë‹ˆì¼€ì´ì…˜ ìŠ¤í‚¬ì„ ëŠ˜ë ¤ê°€ë³´ì„¸ìš”",
        "ì²´ê³„ì ì¸ ë¬¸ì œ í•´ê²° ë°©ë²•ì„ ë°°ì›Œë³´ì‹œë©´ ì¢‹ê² ì–´ìš”"
    ]
    
    if has_invalid_responses:
        learning_directions[1] = "ì˜ë¯¸ìˆëŠ” ë¬¸ì¥ìœ¼ë¡œ ê³ ê°ê³¼ ì†Œí†µí•˜ëŠ” ëŠ¥ë ¥ì„ ê¸°ë¥´ê³ , ìƒí™©ë³„ ì»¤ë®¤ë‹ˆì¼€ì´ì…˜ ìŠ¤í‚¬ì„ ëŠ˜ë ¤ê°€ë³´ì„¸ìš”"
    
    # GPT ì¶”ì²œ ìˆœì„œë¥¼ ì‹¤ì œ ì‹œë‚˜ë¦¬ì˜¤ ë‚´ìš©ìœ¼ë¡œ ë³€í™˜
    formatted_order_list = []
    for i, position in enumerate(gpt_order, 1):
        if position <= len(request.userorder):
            actual_scenario_id = request.userorder[position - 1]
            # âœ… ìˆ˜ì •: requestë¥¼ í•¨ê»˜ ì „ë‹¬
            scenario_info = get_scenario_info_by_id(actual_scenario_id, request)
            formatted_order_list.append(f"{i}ìˆœìœ„: {scenario_info['content']}")
    
    return {
        "userOrder": format_user_order_with_real_ids(request.userorder, request),
        "participationFeedback": participation_feedback,
        "scenarioCoaching": coaching,
        "orderAnalysis": order_analysis,
        "strengths": [
            "ë¹ ë¥¸ ìƒí™© íŒë‹¨ ëŠ¥ë ¥ì„ ê°–ì¶”ê³  ê³„ì„¸ìš”",
            "ì§ì›ì„ ìƒê°í•˜ëŠ” ë”°ëœ»í•œ ë§ˆìŒì´ ë‹ë³´ì—¬ìš”",
            "ê¸°ë³¸ì ì¸ ê³ ê° ì„œë¹„ìŠ¤ ë§ˆì¸ë“œë¥¼ ê°–ì¶”ê³  ìˆì–´ìš”"
        ],
        "learningDirections": learning_directions,
        "gptOrderDetails": {
            "recommendedOrder": gpt_order,
            "formattedOrderList": formatted_order_list
        },
        "gptReasoningDetails": {
            "priorityCriteria": "ë§¤ì¥ ìš´ì˜ì— ë¯¸ì¹˜ëŠ” íŒŒê¸‰íš¨ê³¼ì™€ ê³ ê°ì´ ì§ì ‘ ì²´ê°í•˜ëŠ” ë¶ˆí¸í•¨ì˜ ì‹œê¸‰ì„±ì„ ì¢…í•©ì ìœ¼ë¡œ ê³ ë ¤í•©ë‹ˆë‹¤.",
            "detailedReasoning": "ì»¤í”¼ë¨¸ì‹  ê³ ì¥ì€ ë§¤ì¥ì˜ í•µì‹¬ ê¸°ëŠ¥ì— ì˜í–¥ì„ ë¯¸ì¹˜ë¯€ë¡œ ìµœìš°ì„ ìœ¼ë¡œ ì²˜ë¦¬í•´ì•¼ í•©ë‹ˆë‹¤. ëŒ€ê¸° ê³ ê°ì€ ì¦‰ì‹œ ë³´ì´ëŠ” ë¬¸ì œì´ë¯€ë¡œ ë‘ ë²ˆì§¸ë¡œ ì²˜ë¦¬í•˜ê³ , ì£¼ë¬¸ ì·¨ì†ŒëŠ” ê°œë³„ ê³ ê° ë§Œì¡±ë„ì— ì§ì ‘ ì˜í–¥ì„ ë¯¸ì¹˜ë¯€ë¡œ ì„¸ ë²ˆì§¸ì…ë‹ˆë‹¤. ë°°ë‹¬ ì£¼ë¬¸ì€ ì—¬ëŸ¬ ê³ ê°ì´ ê¸°ë‹¤ë¦¬ê³  ìˆì–´ ë„¤ ë²ˆì§¸ë¡œ ì²˜ë¦¬í•˜ë©°, ì‹ ì… ì§ì› êµìœ¡ì€ ì¥ê¸°ì  ê´€ì ì—ì„œ ì¤‘ìš”í•˜ì§€ë§Œ ì¦‰ì‹œì„±ì´ ìƒëŒ€ì ìœ¼ë¡œ ë‚®ì•„ ë§ˆì§€ë§‰ìœ¼ë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤."
        }
    }

def create_default_educational_analysis(request: EducationalAnalysisRequest):
    """êµ¬ë²„ì „ í˜¸í™˜ì„±ì„ ìœ„í•œ ê¸°ë³¸ í•¨ìˆ˜"""
    return create_improved_default_educational_analysis(request)